---
layout: post
title: "AI Exploration: Part II - OpenAI Capabilities"
date: 2024-12-29
tags: ["ai"]
published: true
---

**Contents**
* TOC
{:toc}

# Problem Statement

Walkthrough OpenAI's current capabilities and take notes

# Documentation Walkthrough
Much of this will be a rote walkthrough of OpenAI's documentation[^1] along with any definitions or interesting points that standout.

#### Overview
* Models    
    * *context window, which refers to the maximum number of tokens that can be used in a single request, inclusive of both input, output, and reasoning tokens.*
    * o = omni
    * GPT-5 - expected to debut in early 2025
    * *The o1 series of models are trained with reinforcement learning to perform complex reasoning. o1 models think before they answer, producing a long internal chain of thought before responding to the user.*
    * *Embeddings are a numerical representation of text that can be used to measure the relatedness between two pieces of text. Embeddings are useful for search, clustering, recommendations, anomaly detection, and classification tasks.*
    * *Moderation models are designed to check whether content complies with OpenAI's usage policies. The models provide classification capabilities that look for content in categories like hate, self-harm, sexual content, violence, and others.*

#### Capabilities
* Text generation
    * gpt-4o-mini - 15 cents per million input tokens (.000015)
    * gpt-4o - 2.50 dollars per million input tokens, (.00025)
    * roles
        * user
        * developer (used to be called system)
        * assistant
    * Retrieval Augmented Generation (RAG) - *might want to include the results of a database query, a text document, or other resources to help the model generate a relevant response*
    * *Messages with the assistant role are presumed to have been generated by the model, perhaps in a previous generation request*
    * *While each text generation request is independent and stateless (unless you are using assistants), you can still implement multi-turn conversations by providing additional messages as parameters to your text generation request. By using alternating user and assistant messages, you can capture the previous state of a conversation in one request to the model.*
* Vision
    * *For safety reasons, we have implemented a system to block the submission of CAPTCHAs.*
* Image generation
* Audio generation
* Text to speech
* Speech to text
* Embeddings
    * *An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness*
    * Follows the same to the mathmatical definition of a vector but in n-dimensional space.
* Moderation
* Reasoning
    * *The o1 models introduce reasoning tokens. The models use these reasoning tokens to "think"*
    * *While reasoning tokens are not visible via the API, they still occupy space in the model's context window and are billed as output tokens.*
    * *Avoid chain-of-thought prompts: Since these models perform reasoning internally, prompting them to "think step by step" or "explain your reasoning" is unnecessary.*
    * *OpenAI o1 series models are able to implement complex algorithms and produce code.*
    * Current o1 models - Available to Tier 5 customers only
* Structure Output
    * *Structured Outputs is a feature that ensures the model will always generate responses that adhere to your supplied JSON Schema*
    * *Structured Outputs is the evolution of JSON mode. While both ensure valid JSON is produced, only Structured Outputs ensure schema adherance.*
    * *When JSON mode is turned on, the model's output is ensured to be valid JSON, except for in some edge cases that you should detect and handle appropriately.*
    * *When using JSON mode, you must always instruct the model to produce JSON via some message in the conversation, for example via your system message.*
    * *Structured Outputs can be enabled by setting the parameter strict: true*
* Predicted Outputs
    * *Predicted Outputs enable you to speed up API responses from Chat Completions when many of the output tokens are known ahead of time.*
    * *Predicted Outputs are particularly useful for regenerating text documents and code files with small modifications. aka code refactoring*
    * *Note that any rejected tokens are still billed like other completion tokens generated by the API, so Predicted Outputs can introduce higher costs for your requests.*
* Function calling
    * *Function calling enables developers to connect language models to external data and systems.*
    * *When you use the OpenAI API with function calling, the model never actually executes functions itself - instead, it simply generates parameters that can be used to call your function. You are then responsible for handling how the function is executed in your code.*
    * *Next we need to provide our function definitions within an array of available “tools” when calling the Chat Completions API.*
        `arguments = json.loads(tool_call['function']['arguments'])`
    * *Under the hood, functions are injected into the system message in a syntax the model has been trained on. This means functions count against the model's context limit and are billed as input tokens.*

#### Realtime
* Realtime
    * *The OpenAI Realtime API enables you to build low-latency, multi-modal conversational experiences with expressive voice-enabled models.*
    * *The developer's server uses a standard API key to request an ephemeral key from the OpenAI REST API, and returns that new key to the browser. Note that ephemeral keys currently expire one minute after being issued.*
    * *While it is technically possible to use a standard API key to authenticate WebRTC sessions, this is a dangerous and insecure practice. Standard API keys grant access to your full OpenAI API account, and should only be used in secure server-side environments. You should use ephemeral keys in client-side applications whenever possible.*
    * *A Realtime session is a stateful interaction between the model and a connected client.*
    * *The maximum duration of a Realtime session is 30 minutes.*
    * *By default, Realtime sessions have voice activity detection (VAD) enabled, which means the API will determine when the user has started or stopped speaking, and automatically start to respond.*
    * *How is audio in Chat Completions different from the Realtime API? The underlying GPT-4o audio model is exactly the same. The Realtime API operates the same model at lower latency.*
* Fine-tuning
    * *Higher quality results than prompting*
    * *Using demonstrations to show how to perform a task is often called "few-shot learning."*
    * *Fine-tuning improves on few-shot learning by training on many more examples than can fit in the prompt, letting you achieve better results on a wide number of tasks. Once a model has been fine-tuned, you won't need to provide as many examples in the prompt.*
    * *We recommend first attempting to get good results with prompt engineering, prompt chaining (breaking complex tasks into multiple prompts), and function calling*
    * *Another scenario where fine-tuning is effective is reducing cost and/or latency by replacing a more expensive model like gpt-4o with a fine-tuned gpt-4o-mini model.*
    * *To fine-tune a model, you are required to provide at least 10 examples.*
    * *Note that we don't charge for tokens used for training validation.*
    * An epoch refers to one complete pass of the entire training dataset through the learning algorithm.[^2]
    * *In addition to creating a final fine-tuned model at the end of each fine-tuning job, OpenAI will create one full model checkpoint for you at the end of each training epoch.*
    * overfitting - machine learning model gives accurate predictions for training data but not for new data.[^3]
    * *(loss should decrease, token accuracy should increase)*
    * *Fine-tuning is also possible with images in your JSONL files.* 
    * JSONL = JSON Lines
    * *Direct Preference Optimization (DPO) fine-tuning allows you to fine-tune models based on prompts and pairs of responses.*
        - *A preferred output (an ideal assistant response).*
	    - *A non-preferred output (a suboptimal assistant response).*
    * *OpenAI offers Supervised Fine-Tuning (SFT) as the default method for fine-tuning jobs.*
    * *function_call and functions have been deprecated in favor of tools it is recommended to use the tools parameter instead.*
    * *OpenAI provides the ability for you to integrate your fine-tuning jobs with 3rd parties via our integration framework.*
    * *Weights and Biases (W&B) is a popular tool for tracking machine learning experiments.*
    * *When should I use fine-tuning vs embeddings / retrieval augmented generation?*
        - *Embeddings with retrieval is best suited for cases when you need to have a large database of documents with relevant context and information.*
* Distillation
    * *Model Distillation allows you to leverage the outputs of a large model to fine-tune a smaller model, enabling it to achieve similar performance on a specific task.*
    * *The first step in the distillation process is to generate good results with a large model*...*store them using the store: true option*
* Evaluations
    * *evals - Similar to having unit tests set up for traditional software*
* Prompt generation
    * *Prompts: We use meta-prompts that incorporate best practices to generate or improve prompts.*
    * *Schemas: We use meta-schemas that produce valid JSON and function syntax.*
    * *However, the official meta-schemas provided by the JSON Schema Specification rely on features not currently supported in strict mode.*
    * *This means we can't currently use strict mode to generate schemas.*
    * *we define a pseudo-meta-schema — a meta-schema that uses features not supported in strict mode to describe only the features that are supported in strict mode.*

#### Assistants
* Assistants
    * *An Assistant has instructions and can leverage models, tools, and files to respond to user queries. The Assistants API currently supports three types of tools: Code Interpreter, File Search, and Function calling.*
    * *The Assistants API is in beta*
    * *Assistants can access persistent Threads.*
    * *Calls to the Assistants API require that you pass a beta HTTP header.*
    * *the file_search tool uses the Vector Store object.*
    * *Vector Store objects give the File Search tool the ability to search your files.*
    * *Adding a file to a vector_store automatically parses, chunks, embeds and stores the file in a vector database that's capable of both keyword and semantic search.*
    * *Code Interpreter allows Assistants to write and run Python code in a sandboxed execution environment.*
    * *Code Interpreter is charged at $0.03 per session*
    * *Each session is active by default for one hour*

#### ChatGPT[^4]
* GPT Actions
    * *GPT Actions are stored in Custom GPTs, which enable users to customize ChatGPT for specific use cases by providing instructions, attaching documents as knowledge, and connecting to 3rd party services.*
    * Need to scroll all the way down and look at the bottom of the page
    * *At their core, GPT Actions leverage Function Calling to execute API calls.*
    * *If the x-openai-isConsequential field is true, ChatGPT treats the operation as "must always prompt the user for confirmation before running" and don't show an "always allow" button*
    * *If the field isn't present, ChatGPT defaults all GET operations to false and all other operations to true*
    * *A GPT Action requires an Open API schema to describe the parameters of the API call*

#### Best Practices
* Prompt Caching
    * *Prompt caches are not shared between organizations. Only members of the same organization can access caches of identical prompts.*
    * *This is because only the prompt itself is cached, while the actual response is computed anew each time based on the cached prompt.*
* Latency optimization
    * *Inference speed - actual rate at which the LLM processes tokens*

# References

[^1]: [https://platform.openai.com/docs/](https://platform.openai.com/docs/)

[^2]: [https://www.appliedaicourse.com/blog/epoch-in-machine-learning/](https://www.appliedaicourse.com/blog/epoch-in-machine-learning/)

[^3]: [https://aws.amazon.com/what-is/overfitting/](https://aws.amazon.com/what-is/overfitting/)

[^4]: InstructGPT is an earlier version of ChatGPT, a fine-tuned GPT-3.5 that is trained to be more truthful and less toxic. InstructGPT was appoximately released on January 27 2022 and ChatGPT was released November 30, 2022. 